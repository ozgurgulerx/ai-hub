# From Seeing to Doing (Fei-Fei Li, NeurIPS) — Notes

Summary for: https://youtu.be/UxDMA8kSLGo  
Generated from transcript tooling (Noiz): https://noiz.io/tools/youtube-subtitles  
Status: **To verify**

## Visual Intelligence as Foundation (Claims)

- Visual intelligence is framed as foundational to broader intelligence (“seeing and perceiving the world”). [Unverified]
- Object recognition is described as an early “north star” in machine vision with neural correlates, shaping early ML/CV. [Unverified]
- Data diversity (not only quantity) is described as critical for generalization, with ImageNet positioned as catalytic for deep learning. [Unverified]

## Beyond Pixels: Relationships and Scene Structure (Claims)

- Visual understanding is described as extending beyond pixels to infer relationships and permanence, using constructs like scene graphs and datasets like Visual Genome. [Unverified]

## From Understanding → Reasoning → Generation (Claims)

- Visual intelligence is described as progressing in stages (understanding → reasoning → generation), with generative models (GANs/VAEs/diffusion) enabling pixel synthesis and edits. [Unverified]

## Spatial Intelligence and 3D (Claims)

- A key claim is that 3D understanding resolves issues like occlusion and interaction limitations that 2D representations can’t address. [Unverified]
- Spatial intelligence is framed as creating a “seeing, learning, doing” loop enabling agents to move and interact. [Unverified]

## Embodied AI Benchmarks and Tooling (Claims)

- A “Behavior benchmark” is described as covering 1000+ everyday tasks across 50 scenes with 10,000 object assets and a simulator for long-horizon robot training. [Unverified]
- “Digital cousins” are described as outperforming “digital twins” by being cheaper and more scalable while preserving geometric/semantic affordances. [Unverified]
- Benchmarks like Clevver and Eagle 4D are described as showing a large gap between current multimodal LLMs and humans in spatial-temporal reasoning. [Unverified]

## Manipulation Data Collection and Learning (Claims)

- DexCap is described as using motion gloves to collect high-resolution finger movement data for tasks like tea serving, transferring demonstrations to robotic arms. [Unverified]
- Voxposer and ReCAP are described as leveraging LLMs/VLMs for zero-shot learning of real-world manipulation tasks using motion maps and spatial-temporal planning. [Unverified]

## Healthcare and Assistive Interfaces (Claims)

- Computer vision sensors are described as verifying hand hygiene in hospitals; hospital-acquired infections are described as causing major mortality. [Unverified]
- Non-invasive EEG caps are described as decoding brain signals to control robots for daily tasks. [Unverified]

## Generative 3D Environments (Claims)

- World Labs is described as generating navigable 3D environments from text or reference images with multi-view consistency. [Unverified]

## Related (In This Repo)

- Multimodal models: `../../multi-modal-models/README.md`
- World-model planning notes: `../README.md`

