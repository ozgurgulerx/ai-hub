# Jason Wei (OpenAI): Some Intuitions About Large Language Models — Notes

Summary for: https://youtu.be/l898fqkjdFc  
Generated from subtitles tooling (Noiz): https://noiz.io/tools/youtube-subtitles  
Status: **To verify**

## Scaling and Performance (As Described)

- LLMs are described as showing reliable improvements with scaling: loss decreases predictably across seven orders of magnitude of compute. [Unverified]
- Next-word prediction is described as learning millions of tasks simultaneously (grammar, world knowledge, translation, spatial reasoning). [Unverified]

## Emergent Abilities and Benchmarking

- Emergent abilities are described as making model comparison harder and motivating new benchmarks to measure expanding capabilities. [Unverified]
- Downstream tasks are described as improving at different rates with scale:
  - some (e.g., math) improve sharply after thresholds,
  - others may plateau. [Unverified]

## AI Research Paradigm Shift

- A “general scaling law” framing is described as implying progress is driven largely by investment, time, and researcher count. [Unverified]
- A paradigm shift is described toward large-scale collaboration and engineering-heavy approaches to train models with massive compute and data. [Unverified]

## Related (In This Repo)

- Jagged intelligence / verification framing (Jason Wei; Noiz summary, to verify): `jason-wei-3-key-ideas-in-ai-2025-noiz-summary.md`
- Inference engineering (serving/scale considerations): `../../../inference-engineering/README.md`

