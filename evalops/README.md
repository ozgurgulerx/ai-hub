# EvalOps

This section is for evaluation design and operating practices: how to define success, build test suites, run regressions, and connect eval results to product and business outcomes.

## Suggested Structure (Planned)

- `notes/` — talk notes and distilled heuristics (labeled **To verify** when sourced from transcripts/summaries)
- `rubrics/` — reusable grading rubrics and criteria
- `suites/` — eval suites by capability (RAG, agents, safety, extraction, etc.)
- `metrics/` — metric definitions and thresholds

## Notes

- Evaluation design (reverse from outcomes; Noiz summary, to verify): `notes/evaluation-design-for-reliable-ai-agents-noiz-summary.md`
- Safety benchmarking (multimodal; Noiz summary, to verify): `notes/safety-benchmarking-testing-multimodal-llms-tianwei-zhang-noiz-summary.md`
- DSPy / compound AI systems + optimization loop (Noiz summary, to verify): `notes/fireside-chat-dspy-creator-omar-khattab-noiz-summary.md`
- Five hard-earned eval lessons (Braintrust; Noiz summary, to verify): `notes/five-hard-earned-lessons-about-evals-ankur-goyal-braintrust-noiz-summary.md`
- Data quality + verifiers + scalable oversight (SurgeAI; Noiz summary, to verify): `notes/no-priors-124-surgeai-edwin-chen-data-quality-and-verifiers-noiz-summary.md`
