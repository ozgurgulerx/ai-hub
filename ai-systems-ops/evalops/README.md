# EvalOps

This section is for evaluation design and operating practices: how to define success, build test suites, run regressions, and connect eval results to product and business outcomes.

## Suggested Structure (Planned)

- `notes/` — talk notes and distilled heuristics (labeled **To verify** when sourced from transcripts/summaries)
- `rubrics/` — reusable grading rubrics and criteria
- `suites/` — eval suites by capability (RAG, agents, safety, extraction, etc.)
- `metrics/` — metric definitions and thresholds

## Notes

**[EvalOps Comprehensive Guide](notes/evalops-comprehensive-guide.md)** — Complete reference covering:
- Outcome-first evaluation design
- Data quality and synthetic data limitations
- LLM-assisted evaluation
- DSPy and compound AI systems
- Context optimization
- Safety benchmarking
- Scalable oversight patterns
