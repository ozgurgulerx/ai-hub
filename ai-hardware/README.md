# AI Hardware

Notes and practical references for the hardware that powers modern AI systems: GPUs/accelerators, memory/bandwidth, interconnects, and deployment trade-offs.

## Topics (planned)

- GPU/accelerator fundamentals (compute vs memory vs bandwidth)
- Training vs inference hardware trade-offs
- Serving hardware: latency, throughput, batching, KV cache memory
- Multi-GPU: PCIe vs NVLink/NVSwitch, topology, comms bottlenecks
- Datacenter considerations: networking, storage, and cost

