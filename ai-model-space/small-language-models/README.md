# Small Language Models (SLMs)

Notes and building guides for **small language models**: compact LMs (often framed as “< 1B parameters”) that trade peak capability for lower latency/cost and easier deployment.

## What’s Here

- Build an SLM from scratch (TinyStories + GPT-2 tokenizer + nanoGPT-style pipeline): `notes/build-slm-from-scratch-tinystories.md`

## Related (In This Repo)

- Inference & serving tradeoffs: `../../inference-engineering/README.md`
- Continual updates / distillation patterns: `../../reinforcement-learning/continual-learning/README.md`
- Post-training & fine-tuning overview (MIT 6.S191 / Liquid AI; Noiz summary, to verify): `../../reinforcement-learning/planning/post-training-and-fine-tuning-mit-6s191-liquid-ai-noiz-summary.md`
- Retrieval + grounding to cover knowledge gaps: `../../retrieval-augmented-systems/README.md`
