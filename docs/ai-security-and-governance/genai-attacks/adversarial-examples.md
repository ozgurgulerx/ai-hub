# Adversarial Examples

Carefully crafted inputs designed to cause model misclassification or unexpected behavior.

## Types

- **Perturbation attacks**: Small, often imperceptible changes to inputs
- **Patch attacks**: Visible adversarial patches in images
- **Universal adversarial perturbations**: Single perturbation that works across many inputs
- **Text adversarial examples**: Character/word substitutions that fool NLP models

## Attack Scenarios

- Bypass content moderation
- Evade malware detection
- Fool autonomous systems
- Manipulate recommendations

## Mitigations

- Adversarial training
- Input preprocessing and detection
- Ensemble methods
- Certified defenses
- Robustness testing
