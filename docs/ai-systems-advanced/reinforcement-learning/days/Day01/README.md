# Day 01 — The Alignment Gate (RLFT/RFT on Azure AI Foundry)

Day 01 is designed to make you operational on **Azure AI Foundry Reinforcement Fine-Tuning (RLFT / RFT)** in one sitting: data contract, graders, structured outputs, job ops, cost controls, and *how to debug from curves*.

If you ship the artifacts and pass the criteria below, you can credibly run RLFT in real projects.

<details>
<summary><strong>Quick navigation</strong></summary>

- [The real-world lab](#the-real-world-lab-what-youre-building)
- [Outputs to ship](#outputs-to-ship-commit-these)
- [Pass/fail](#passfail-no-excuses)
- [What you must learn](#what-you-must-learn-rlft-scope-only)
- [One-day schedule](#one-day-schedule-gradual-operational)
- [Ready-made Python scripts](#ready-made-python-scripts-already-in-repo-also-shown-here)
- [Highest-signal references](#highest-signal-references-rlft-only)
- [Next](#next)
</details>

---

## The real-world lab (what you’re building)

You’re building a **Support Ticket Router** that must output **strict JSON**:

- choose `category` and `priority`
- choose `route_to` (queue)
- decide `needs_human`

This is a real RLFT-shaped problem because the reward is **verifiable** (schema + exact-match fields + rule checks), and you can harden it against reward hacking.

---

## Outputs to ship (commit these)

Minimum artifact set (included in repo + generated by the scripts below):

```
days/Day01/
├─ README.md
├─ LAB.md
├─ THEORY.md
├─ TIER01.log
├─ TIER02.log
├─ TIER03.log
├─ bootstrap_day01_rlft.py
├─ data/
│  ├─ rft_train.jsonl
│  └─ rft_valid.jsonl
├─ graders/
│  └─ python_grader.py
├─ schema/
│  └─ response_format.json
└─ eval/
   ├─ smoke_test_payload.json
   ├─ local_smoke_test.py
   └─ analyze_results_csv.py
```

Foundry evidence to capture (screenshots or exported CSVs in `days/Day01/eval/`):

- [ ] RLFT job id + model version + region/tier
- [ ] reward curves (train/valid)
- [ ] reasoning-token curves (train/valid)
- [ ] at least one auto-eval snapshot created during training ([Microsoft Learn][1])

---

## Pass/fail (no excuses)

You **pass Day 01** only if all are true:

- [ ] Foundry accepts `data/rft_train.jsonl` and `data/rft_valid.jsonl` (validator passes).
- [ ] Your `graders/python_grader.py` returns a scalar reward and has **>= 3 explicit anti-cheat checks**.
- [ ] You launched an RLFT job **or** documented the blocker precisely (model/region access), including the attempted region/tier and what you tried next. ([Microsoft Learn][1])
- [ ] You captured the key curves: `train_reward_mean`, `valid_reward_mean`, `train_reasoning_tokens_mean`, `valid_reasoning_tokens_mean`. ([Microsoft Learn][1])
- [ ] You wrote a 10–20 line postmortem in `TIER01.log` stating: failure mode → fix → evidence.

You **fail Day 01** if any are true:

- [ ] No validation set (RLFT requires it). ([Microsoft Learn][1])
- [ ] Your outputs aren’t strictly structured (you “parse whatever”) and your grader can be hacked by formatting/verbosity.
- [ ] You can’t explain the mapping: JSONL fields → `item.*` → templating → grader inputs. ([Microsoft Learn][1])

---

## What you must learn (RLFT scope only)

### 1) Availability and constraints (model/region/tier)

- RLFT model support is specific (e.g., `o4-mini-2025-04-16`; `gpt-5-2025-08-07` may be preview/private). ([Microsoft Learn][1])
- Availability is **region-scoped**; treat “model not available” as a first-class failure mode and document the fallback (switch region/tier). ([Microsoft Learn][4])

### 2) Data contract (what Foundry validates)

- JSONL with `messages` array; last message must be `user`. ([Microsoft Learn][1])
- Provide both train + validation files. ([Microsoft Learn][1])
- Use UTF-8 with BOM; file size limits apply. ([Microsoft Learn][4])
- Extra fields become `item.*` (used by templating and graders). ([Microsoft Learn][1])
- Reserved vs free-form keys: treat `messages` (and `tools`, if used) as reserved; everything else should be safe as `item.*`. ([Microsoft Learn][1])

### 3) Output contract (`response_format` JSON schema)

- Enforce strict JSON schema so you can grade `sample.output_json` instead of brittle text parsing. ([Microsoft Learn][1])
- Make it strict on purpose: `required` fields only + `additionalProperties: false` = smaller hack surface.

### 4) Graders (the moat)

You must understand the full taxonomy and how it composes: ([Microsoft Learn][1])

- `python` (deterministic, best for verifiable tasks)
- `string_check` / `text_similarity` (fast constraints / similarity)
- `score_model` (LLM-as-judge; no deployment required)
- `multigrader` (combine scores with arithmetic; gate style behind correctness)
- endpoint grader (preview; external scoring)

**Python grader contract (memorize)** ([Microsoft Learn][1])

- implement `grade(sample, item) -> float`
- fail closed on parse/eval errors (don’t “best effort” your way into reward hacking)
- design for sandbox limits (no network; tight runtime/memory; small code package) ([Microsoft Learn][1])

**Templating mental model (memorize)** ([Microsoft Learn][1])

- your JSONL row becomes `item.*` (e.g., `item.label_category`)
- model outputs are available as `sample.output_text` and (with schema) `sample.output_json`

### 5) Hyperparams + signals + debugging

- RLFT-specific knobs: `eval_interval`, `eval_samples`, `compute_multiplier`, `reasoning_effort`. ([Microsoft Learn][1])
- Interpretations (good enough for Day 01):
  - `eval_interval`: how often auto-evals run
  - `eval_samples`: how many eval rollouts get scored
  - `compute_multiplier`: how much training compute you buy (cost/throughput)
  - `reasoning_effort`: nudges reasoning-token behavior (watch token curves)
- First dashboard literacy: reward curves + reasoning-token curves; train↑ / valid↓ often means reward hacking or distribution shift. ([Microsoft Learn][1])
- Auto-evals: Foundry runs evals at `eval_interval`; use them for early stopping decisions. ([Microsoft Learn][1])

### 6) Job lifecycle + cost controls

- Checkpoints, pause/resume, events, results artifacts. ([Microsoft Learn][4])
- Cost model and the **$5k auto-pause** behavior; grader-token costs matter if you add model graders. ([Microsoft Learn][3])

---

## One-day schedule (gradual, operational)

If you want the minimal “field drill” version of Day 01, use `days/Day01/LAB.md`.

### Block 0 — Foundry pre-flight (30–45m)

Goal: remove “platform unknowns” first.

- Confirm you can reach **Build → Fine-tune** and upload data. ([Microsoft Learn][4])
- Confirm the RLFT model is selectable in your region/tier; record the result in `TIER01.log`.
- Set budget/alerts; assume costs will surprise you if you don’t. ([Microsoft Learn][3])

### Block 1 — Generate RLFT data + schema (30–45m)

Goal: produce validator-passing JSONL + strict schema.

- Run `bootstrap_day01_rlft.py` (below) to generate:
  - `data/rft_train.jsonl`, `data/rft_valid.jsonl` (UTF-8 with BOM)
  - `schema/response_format.json` (strict JSON schema)
  - `eval/smoke_test_payload.json` (one item + sample outputs for local grading)

### Block 2 — Sanity-check the grader locally (20–30m)

Goal: prove your reward function is not vibes.

- Run `python3 days/Day01/eval/local_smoke_test.py`
- Confirm:
  - invalid JSON -> reward 0
  - wrong label -> reward low
  - correct labels -> reward high
  - anti-cheat triggers behave as expected

### Block 3 — Configure and launch RLFT in Foundry (30–45m)

Goal: launch a job you can debug.

- Foundry UI checklist (high-signal, no fluff):
  - Build → Fine-tune → Reinforcement fine-tuning (RLFT/RFT). ([Microsoft Learn][1])
  - Pick the base model **version string** (don’t write “latest”). ([Microsoft Learn][1])
  - Upload both train + validation JSONL. ([Microsoft Learn][1])
  - Set `response_format` using `schema/response_format.json` (strict). ([Microsoft Learn][1])
  - Add the Python grader using `graders/python_grader.py`. ([Microsoft Learn][1])
  - Leave RLFT hyperparams on `auto` for the first run; change only with a hypothesis. ([Microsoft Learn][1])
  - Start job; immediately record job id + region/tier + config in `TIER01.log`.

### Block 4 — Monitor curves + run one deliberate hack (45–60m)

Goal: learn by breaking it.

- Watch reward + reasoning-token curves. ([Microsoft Learn][1])
- Deliberately introduce a hack (example): loosen schema or stop failing closed.
- Observe train/valid divergence.
- Fix it (tighten schema + anti-cheat), and record before/after in `TIER01.log`.

### Block 5 — Ops drill (15–30m)

Goal: be able to operate RLFT jobs.

- Find job events (UI/REST) and identify one meaningful warning/error. ([Microsoft Learn][4])
- Pause/resume once on purpose (know what “deployable checkpoint” means in your workflow). ([Microsoft Learn][4])
- Export/download results artifacts and save them under `eval/`.

---

## Ready-made Python scripts (already in repo, also shown here)

Run order (from repo root):

- Generate train/valid + schema + smoke payload: `python3 days/Day01/bootstrap_day01_rlft.py`
- Verify the grader locally: `python3 days/Day01/eval/local_smoke_test.py`
- Summarize a Foundry `results.csv` export: `python3 days/Day01/eval/analyze_results_csv.py /path/to/results.csv`

<details>
<summary><code>days/Day01/bootstrap_day01_rlft.py</code></summary>

```python
from __future__ import annotations

import json
import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional


REPO_ROOT = Path(__file__).resolve().parents[2]
DAY_DIR = REPO_ROOT / "days" / "Day01"


@dataclass(frozen=True)
class Ticket:
    text: str
    category: str
    priority: str
    route_to: str
    needs_human: bool


CATEGORIES = ["billing", "login", "bug", "feature_request", "security", "account_closure"]
PRIORITIES = ["p0", "p1", "p2", "p3"]
ROUTES = ["support_l1", "support_l2", "billing_team", "security_team", "retention_team"]


def _route_rules(category: str, priority: str) -> str:
    if category == "security":
        return "security_team"
    if category == "billing":
        return "billing_team"
    if category == "account_closure":
        return "retention_team"
    if priority in {"p0", "p1"}:
        return "support_l2"
    return "support_l1"


def _needs_human_rules(category: str, priority: str, text: str) -> bool:
    if category in {"security", "account_closure"}:
        return True
    if priority == "p0":
        return True
    if "lawsuit" in text.lower() or "attorney" in text.lower():
        return True
    return False


def make_ticket(rng: random.Random) -> Ticket:
    templates = [
        ("I can't log into my account. It says my password is wrong.", "login", "p2"),
        ("My card was charged twice for the same invoice. Please refund.", "billing", "p1"),
        ("The app crashes when I click Export. Steps: open report -> export -> crash.", "bug", "p1"),
        ("Can you add SSO support for our org?", "feature_request", "p3"),
        ("I think my account was hacked. I see logins from new countries.", "security", "p0"),
        ("Please close my account and delete my data.", "account_closure", "p2"),
        ("We were billed for 12 seats but only have 9 users. Fix this.", "billing", "p2"),
        ("Password reset email never arrives; I've tried 3 times.", "login", "p2"),
        ("Your API returns 500 on /v1/export. This blocks our nightly job.", "bug", "p0"),
        ("Feature request: export to parquet for our data pipeline.", "feature_request", "p3"),
        ("Suspicious MFA prompts started today; I didn't request them.", "security", "p0"),
        ("Close my account immediately or my attorney will contact you.", "account_closure", "p1"),
    ]
    text, category, priority = rng.choice(templates)
    route_to = _route_rules(category, priority)
    needs_human = _needs_human_rules(category, priority, text)
    return Ticket(
        text=text, category=category, priority=priority, route_to=route_to, needs_human=needs_human
    )


def ticket_to_item(ticket: Ticket) -> Dict[str, Any]:
    return {
        "messages": [
            {
                "role": "developer",
                "content": (
                    "You are a support-triage router. Output STRICT JSON only. "
                    "Do not include prose. Follow the JSON schema."
                ),
            },
            {"role": "user", "content": ticket.text},
        ],
        "label_category": ticket.category,
        "label_priority": ticket.priority,
        "label_route_to": ticket.route_to,
        "label_needs_human": ticket.needs_human,
        "allowed_categories": CATEGORIES,
        "allowed_priorities": PRIORITIES,
        "allowed_routes": ROUTES,
    }


def write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8-sig") as f:
        for row in rows:
            f.write(json.dumps(row, ensure_ascii=False) + "\n")


def _safe_read_first_row(jsonl_path: Path) -> Optional[Dict[str, Any]]:
    if not jsonl_path.exists():
        return None
    with jsonl_path.open("r", encoding="utf-8-sig") as f:
        line = f.readline().strip()
    return json.loads(line) if line else None


def main() -> None:
    rng = random.Random(7)
    n_train, n_valid = 240, 60

    tickets = [make_ticket(rng) for _ in range(n_train + n_valid)]
    train_rows = [ticket_to_item(t) for t in tickets[:n_train]]
    valid_rows = [ticket_to_item(t) for t in tickets[n_train:]]

    write_jsonl(DAY_DIR / "data" / "rft_train.jsonl", train_rows)
    write_jsonl(DAY_DIR / "data" / "rft_valid.jsonl", valid_rows)

    response_format = {
        "name": "support_ticket_route",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "category": {"type": "string"},
                "priority": {"type": "string"},
                "route_to": {"type": "string"},
                "needs_human": {"type": "boolean"},
            },
            "required": ["category", "priority", "route_to", "needs_human"],
            "additionalProperties": False,
        },
    }
    (DAY_DIR / "schema").mkdir(parents=True, exist_ok=True)
    (DAY_DIR / "schema" / "response_format.json").write_text(
        json.dumps(response_format, indent=2), encoding="utf-8"
    )

    (DAY_DIR / "eval").mkdir(parents=True, exist_ok=True)
    first_valid_item = _safe_read_first_row(DAY_DIR / "data" / "rft_valid.jsonl") or valid_rows[0]
    good_output = {
        "category": first_valid_item["label_category"],
        "priority": first_valid_item["label_priority"],
        "route_to": first_valid_item["label_route_to"],
        "needs_human": first_valid_item["label_needs_human"],
    }
    bad_category = next(
        c for c in first_valid_item["allowed_categories"] if c != first_valid_item["label_category"]
    )
    bad_output = {**good_output, "category": bad_category}
    (DAY_DIR / "eval" / "smoke_test_payload.json").write_text(
        json.dumps(
            {
                "item": first_valid_item,
                "samples": [
                    {"output_text": json.dumps(good_output, separators=(",", ":"))},
                    {"output_text": json.dumps(bad_output, separators=(",", ":"))},
                    {"output_text": "not json"},
                ],
            },
            indent=2,
        ),
        encoding="utf-8",
    )

    print("Generated Day01 artifacts in:", DAY_DIR)


if __name__ == "__main__":
    os.chdir(REPO_ROOT)
    main()
```

</details>

<details>
<summary><code>days/Day01/graders/python_grader.py</code></summary>

```python
import json
from typing import Any, Dict, Optional


def _fail(score: float, reason: str) -> float:
    print(f"[grade] {reason}")
    return score


def _get_output_json(sample: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    output_json = sample.get("output_json")
    if isinstance(output_json, dict):
        return output_json

    output_text = sample.get("output_text", "")
    if not isinstance(output_text, str) or not output_text.strip():
        return None
    try:
        return json.loads(output_text)
    except Exception:
        return None


def grade(sample: Dict[str, Any], item: Dict[str, Any]) -> float:
    output = _get_output_json(sample)
    if output is None:
        return _fail(0.0, "invalid_json")

    required_keys = {"category", "priority", "route_to", "needs_human"}
    if set(output.keys()) != required_keys:
        return _fail(0.0, f"bad_keys: {sorted(output.keys())}")

    allowed_categories = set(item.get("allowed_categories", []))
    allowed_priorities = set(item.get("allowed_priorities", []))
    allowed_routes = set(item.get("allowed_routes", []))

    category = output.get("category")
    priority = output.get("priority")
    route_to = output.get("route_to")
    needs_human = output.get("needs_human")

    if category not in allowed_categories:
        return _fail(0.0, f"bad_category: {category}")
    if priority not in allowed_priorities:
        return _fail(0.0, f"bad_priority: {priority}")
    if route_to not in allowed_routes:
        return _fail(0.0, f"bad_route_to: {route_to}")
    if not isinstance(needs_human, bool):
        return _fail(0.0, "needs_human_not_bool")

    required_labels = ("label_category", "label_priority", "label_route_to", "label_needs_human")
    if any(k not in item for k in required_labels):
        return _fail(0.0, "missing_labels_in_item")

    score = 0.0
    score += 0.40 if category == item.get("label_category") else 0.0
    score += 0.30 if priority == item.get("label_priority") else 0.0
    score += 0.20 if route_to == item.get("label_route_to") else 0.0
    score += 0.10 if needs_human == item.get("label_needs_human") else 0.0
    return float(score)
```

</details>

<details>
<summary><code>days/Day01/eval/local_smoke_test.py</code></summary>

```python
import json
import sys
from pathlib import Path


def main() -> None:
    day_dir = Path(__file__).resolve().parents[1]
    sys.path.insert(0, str(day_dir))

    from graders.python_grader import grade  # noqa: WPS433 (runtime import for portability)

    payload = json.loads(
        (Path(__file__).resolve().parent / "smoke_test_payload.json").read_text(encoding="utf-8")
    )
    item = payload["item"]
    samples = payload["samples"]

    for i, sample in enumerate(samples):
        score = grade(sample=sample, item=item)
        print(f"sample[{i}] score={score}")


if __name__ == "__main__":
    main()
```

</details>

<details>
<summary><code>days/Day01/eval/analyze_results_csv.py</code></summary>

```python
from __future__ import annotations

import csv
import math
import sys
from pathlib import Path
from typing import Dict, List, Optional


NUMERIC_FIELDS = {
    "train_reward_mean",
    "valid_reward_mean",
    "train_reasoning_tokens_mean",
    "valid_reasoning_tokens_mean",
}


def _to_float(value: str) -> Optional[float]:
    try:
        x = float(value)
    except Exception:
        return None
    return None if math.isnan(x) else x


def read_csv(path: Path) -> List[Dict[str, str]]:
    with path.open("r", encoding="utf-8-sig", newline="") as f:
        return list(csv.DictReader(f))


def summarize(rows: List[Dict[str, str]]) -> None:
    if not rows:
        print("No rows.")
        return

    fields = rows[0].keys()
    available = [f for f in NUMERIC_FIELDS if f in fields]
    if not available:
        print("No expected numeric fields found. Available columns:")
        for name in sorted(fields):
            print("-", name)
        return

    print(f"rows={len(rows)}")
    for field in available:
        values = [_to_float(r.get(field, "")) for r in rows]
        values = [v for v in values if v is not None]
        if not values:
            print(f"{field}: no numeric values")
            continue
        print(f"{field}: last={values[-1]:.4g} max={max(values):.4g} min={min(values):.4g}")


def main(argv: List[str]) -> None:
    if len(argv) < 2:
        print("Usage: python analyze_results_csv.py /path/to/results.csv")
        raise SystemExit(2)

    path = Path(argv[1]).expanduser().resolve()
    rows = read_csv(path)
    summarize(rows)


if __name__ == "__main__":
    main(sys.argv)
```

</details>

---

## Highest-signal references (RLFT only)

- RLFT/RFT how-to (models, data rules, graders, metrics, response_format, hyperparams): [Microsoft Learn][1]
- Foundry fine-tuning workflow (upload, checkpoints, pause/resume, artifacts, events): [Microsoft Learn][4]
- Cost management (billing model, what’s billed, guardrails, auto-pause): [Microsoft Learn][3]
- RLFT mental model (sample → grade → update; why verifiable tasks matter): [OpenAI docs][5]
- RLFT concepts + use-case map (companion note): `../../../../docs/ai-systems-advanced/reinforcement-learning/planning/rlft_concepts.md`

---

## Next

Day 02 covers DPO + PPO (Azure DPO, PPO as RLHF baseline). Day 03 covers GRPO (RLVR/RFT).

[1]: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reinforcement-fine-tuning?view=foundry-classic "Reinforcement fine-tuning - Microsoft Foundry | Microsoft Learn"
[3]: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tuning-cost-management?view=foundry-classic "Fine-tuning cost management - Azure OpenAI | Microsoft Learn"
[4]: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tuning?view=foundry-classic "Customize a model with Microsoft Foundry fine-tuning - Azure OpenAI | Microsoft Learn"
[5]: https://platform.openai.com/docs/guides/reinforcement-fine-tuning "Reinforcement fine-tuning | OpenAI API"
