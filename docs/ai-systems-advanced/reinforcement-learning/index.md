# Reinforcement Learning

RL for LLMs, world models, and multimodal agents.

## Focus Areas

- **RL for LLMs**: preference optimization, policy gradients, reward modeling
- **Continual learning**: update models without catastrophic forgetting
- **World models**: latent dynamics, imagination-based planning
- **Alignment and safety**: controllability, calibration, robustness

## Learning Path

| Day | Topic |
|-----|-------|
| Day 01 | RLFT/RFT on Azure AI Foundry |
| Day 02 | DPO + PPO baseline |
| Day 03 | GRPO (RLVR/RFT) |

## Notes

- [RLHF Book notes](planning/rlhf_book_notes.md)
- [RLFT concepts](planning/rlft_concepts.md)
- [Post-training & fine-tuning (MIT 6.S191)](planning/post-training-and-fine-tuning-mit-6s191-liquid-ai-noiz-summary.md)
