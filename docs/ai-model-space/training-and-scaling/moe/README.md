# Mixture-of-Experts (MoE)

MoE models trade **quality-per-dollar** by activating only a subset of parameters (“experts”) per token.

## Notes

- Why training MoEs is hard (practitioner write-up): `why-training-moes-is-so-hard.md`

