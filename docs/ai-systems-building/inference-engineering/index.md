# Inference Engineering

Building, scaling, and optimizing LLM inference stacks.

## Topics

- vLLM, TensorRT-LLM, Triton
- Quantization (INT8, INT4, FP8)
- KV-cache, continuous batching
- Speculative decoding
- Latency, throughput, cost trade-offs
