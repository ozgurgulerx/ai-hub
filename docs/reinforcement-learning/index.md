# Reinforcement Learning

RL for LLMs, world models, and multimodal agents.

## Focus Areas

- **RL for LLMs** — Preference optimization, policy gradients, reward modeling
- **Continual Learning** — Update models without catastrophic forgetting
- **World Models** — Latent dynamics, imagination-based planning
- **Multimodal Agents** — Vision-language grounding, tool use, memory
- **Alignment & Safety** — Controllability, calibration, robustness

## Key Concepts

| Concept | Description |
|---------|-------------|
| **RLHF** | Reinforcement Learning from Human Feedback |
| **DPO** | Direct Preference Optimization |
| **PPO** | Proximal Policy Optimization |
| **GRPO** | Group Relative Policy Optimization |
| **Reward Model** | Model that scores outputs for RL training |

## Learning Path

1. **Fundamentals** — MDPs, policy/value functions, PPO
2. **RL for LLMs** — Reward modeling, preference datasets, KL-regularized objectives
3. **World Models** — Latent state-space models, model-based RL
4. **Multimodal Agents** — Vision-language encoders, tool-augmented agents
5. **Evaluation** — Automatic evals, human evals, safety benchmarks
