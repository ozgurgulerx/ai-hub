# DeepSeek V3.2 (Secondary writeup notes — Dec 2025)

This note preserves and distills claims from a third-party writeup (“DeepSeek V3.2 Just Changed the AI Race — Goodbye GPT-5, Gemini 3, and Claude”, dated Dec 3, 2025).

No primary technical report/model card was included in the source excerpt. Treat all items here as **unverified** until corroborated with official documentation and reproducible evaluations.

## Distilled claims (Unverified)

### Positioning

- Described as an “open-source LLM” and “free to use” (verify license/terms).
- Positioned as strong on long-context reasoning, multi-step reasoning, and agent/tool workflows.

### Long-context + attention mechanism

- Claims **128K-token** context.
- Claims a “dynamic sparse attention” approach using a learned “indexer”:
  - scans the sequence and predicts which past tokens matter
  - selects top-k relevant tokens (≈2,048)
  - runs full attention on those tokens
  - described as reducing scaling from O(L²) toward **O(L × k)**

### Training recipe (sparse attention)

- Claims a two-stage pipeline:
  - **Dense warm-up**: train the indexer to imitate full attention
  - **Sparse training**: switch to sparse attention, align behavior using:
    - language modeling loss
    - KL alignment loss
    - “memory consistency checks”

### Tool-use and “thinking” modes

- Claims a large synthetic “agent training data synthesis” program (“1,800+ environments”, “85k+ complex instructions”).
- Claims tool-use is supported in both “thinking” and “non-thinking” modes, and that “thinking” is integrated with tool-use.

### Variants and evaluation claims

- Claims two variants:
  - “V3.2”: positioned as a balanced default.
  - “V3.2-Speciale”: positioned as max reasoning with higher token usage; described as “API-only” and “no tool-use” (for evaluation/research).
- Mentions competition-style “gold-level” claims (IMO/CMO/ICPC/IOI 2025). Treat as marketing until pinned to official sources and reproducible evals.

### Strengths / limitations (as claimed)

- Claimed strengths: long context, efficiency, multi-step reasoning, tool-use integration, deployability.
- Claimed limitations: weaker world knowledge vs some closed models; verbosity; high-difficulty math still behind; still needs retrieval for recency.

## Verification checklist (What to look for in primary sources)

- Context length and long-context eval methodology (needle-in-haystack, multi-doc synthesis, failure modes).
- Architecture: dense vs MoE; whether “dynamic sparse attention” exists; how top-k selection works; latency/memory tradeoffs.
- Tool-use interface: schemas, constraints, “thinking” mode definition, and how tool outputs are handled.
- Variant definitions: what “Speciale” is; availability; whether tool-use differs.
- License/weights: “open-source” vs “open-weight” vs API-only.
- Benchmark claims: datasets, harness, prompts, scoring, cost/latency, and reproduction instructions.

## Sources

- `https://www.deepseek.com/`
- `https://huggingface.co/deepseek-ai/DeepSeek-V3.2/tree/main`
